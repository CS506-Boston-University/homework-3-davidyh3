{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Ve8gp_SbwU"
      },
      "source": [
        "# Homework 3 (100 Points)\n",
        "\n",
        "The goal of this homework is to get practice with classification."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1 (30 points)\n",
        "\n",
        "This exercise will re-use the [Titanic dataset](https://www.kaggle.com/c/titanic/data) (https://www.kaggle.com/c/titanic/data) from homework 1. Download all files this time.\n",
        "\n",
        "a) Handle the missing values in the datasets. Briefly explain. **-5 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b) Add 2 new features / columns to the datasets that you think might be related to the survival of individuals. Explain. **-5 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c) Split the dataset into training and testing. Train a Decision Tree Classifier using all features you think may be related to survival (justify any that you remove). Set the `max_depth` parameter to an appropriate quantity to reduce the runtime and avoid overfitting. Explain with a plot presenting in x-axis the `max_depth` and in y-axis the accuracy (both training and testing). **- 10 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d) Plot the decision tree. Briefly explains how it works. **- 5 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import plot_tree\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "e) Plot the confusion matrix of the above classifier. Comment on the pitfalls of the model (to help: For the examples that were misclassified, choose two at random and walk through the decision tree with their attributes and try to understand why the misclassification occured). **- 5 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 2 (30pts)\n",
        "\n",
        "Random Forest algorithm is an extension to the decision trees. We make use of multiple decision trees to make a decsion(classification/regression) in Random Forests.\n",
        "\n",
        "Ensemble modelling is a method of constructing a strong model using several weak models. Random Forests algorithm is one such ensemble model\n",
        "\n",
        "Multiple small decision trees trained on random parts of the training data collectively make a decision on an input point. The number of trees in this Random Forest algorithm is a hyper-parameter that you need to finetune to get the best output from this model. \n",
        "\n",
        "Every time you construct a tree, you pick random samples of size 'k'(k samples) from the total dataset and construct the tree. Make sure your tree depth is not very high, or the individual tree could overfit to the data. We want the collective model to generalize well to multiple datasets. \n",
        "\n",
        "During classification, we consider the decision of the majority of trees to be the final decision. \n",
        "\n",
        "In this task, you are asked to construct a random forests algorithm on the 'Titanic' dataset making use of your decision trees from **1c** and get the classification outputs.\n",
        "\n",
        "**Note**: Your cannot use the random forests model from sklearn or any other library, but you can use any library that implements an individual decision tree."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a) Construct a Random Forest Classifier using the template below. Use it on the training set you defined earlier with an arbitrary number of trees and tree depth and evaluate it on the test set you defined earlier. **-- 10pts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class Random_Forest():\n",
        "\n",
        "    def __init__(self, num_trees, max_depth):\n",
        "        self.max_depth = max_depth\n",
        "        self.decision_trees = [... for _ in range(num_trees)]\n",
        "\n",
        "    def _fit_tree(self, X, y, idx):\n",
        "        self.decision_trees[idx].fit(X, y)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for idx in range(len(self.decision_trees)):\n",
        "            X_sample, y_sample = self._sample(X, y)\n",
        "            self._fit_tree(X_sample, y_sample, idx)\n",
        "    \n",
        "    def _majority(predictions):\n",
        "        # your code here\n",
        "        return \n",
        "    \n",
        "    def predict(self, X_test, y_test):\n",
        "        # your code here\n",
        "        predictions = ...\n",
        "        return self._majority(predictions)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b) Experiment with\n",
        "\n",
        " - different max depths = 5,10,15\n",
        " - different number of trees [10,50,100,500,1000]\n",
        " - different information criteria ['gini index', 'entropy'] \n",
        "\n",
        "Report the accuracy of your best and worst models, and compare them with the accuracy of the decision tree from **1c**. What observations did you make on Random Forests as a whole? **-- 15pts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c)\n",
        "\n",
        "$$ \\text{Information gain} = \\text{(Entropy of Parent Node)} - \\sum_{c}{\\text{(entropy of child node)} }$$\n",
        "\n",
        "$$Entropy = \\sum_{i=1}^{C} -p_{i}log(p_{i})$$\n",
        "$$p_{i} = \\frac{C_{i}}{ \\sum_{j=1}^{C} C_{j}}$$\n",
        "$C_{i} = \\text{Count of elements belonging to class 'i'}$\n",
        "$C = \\text{Total Number of Elements}$\n",
        "\n",
        "Consider we have a total of 50,000 samples\n",
        "\n",
        "- We randomly sampled about 5000 samples and trained a decision tree. \n",
        "- We are trying to classify the samples at a node for one of the tree 'T'\n",
        "- Parent Node has 34 samples, out of which 15 are positive, and 19 are negative \n",
        "- We divided the node based on a particular column, and now we have two child nodes\n",
        "- Child 1 has 13 samples, of which 9 are positive and 4 are negative\n",
        "- Child 2 has 21 samples, of which 6 are positive and 15 are negative,\n",
        "\n",
        "Calculate the information gain for splitting the parent note based on that column. **--5pts**\n",
        "\n",
        "**Note:** You can attach a handwritten image for this part or write your answer in the markdown cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3 (40 Points)\n",
        "\n",
        "For this question we will use Sonar dataset from sklearn.datasets, which contains sonar signals for classifying objects as either \"rock\" or \"mine.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "sonar = fetch_openml(name=\"sonar\", version=1)\n",
        "\n",
        "X = sonar.data  # Features\n",
        "y = sonar.target  # Target (rock or mine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a) Begin by creating a training and testing datasest from the dataset, with a 80-20 ratio, and random_state=1. **1 pt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b) Train a KNN classifier on the training set to classify sonar signals as either \"Rock\" or \"Mine.\" Use cross-validation to find an appropriate value of K. Evaluate and print the model's performance on the testing set using accuracy. **-- 9 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c) Using any combination of the classification tools we've discussed in class:\n",
        "\n",
        "- KNN\n",
        "- Naive Bayes\n",
        "- SVM\n",
        "- Decision Tree (including Random Forests)\n",
        "- Ensemble Methods (AdaBoost, Bagging)\n",
        "\n",
        "You may also use feature extraction tools like PCA. Train and tune a model on the training set and evaluate its performance on the test set using accuracy. **-- 30 points**\n",
        "\n",
        " * accuracy > .95 **-- 30 points**\n",
        " * accuracy between 0.94 and 0.95 **-- 25 points**\n",
        " * accuracy between 0.92 and 0.94 **-- 20 points**\n",
        " * accuracy between 0.9 and 0.92 **-- 15 points**\n",
        " * accuracy between 0.85 and 0.9 **-- 10 points**\n",
        " * accuracy between 0.8 and 0.85 **-- 7 points**\n",
        " * accuracy between 0.7 and 0.8 **-- 5 points**\n",
        " * accuracy < 0.7 **-- 3 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus (15pts)\n",
        "\n",
        "In this bonus we will implement 1-dimensional GMM clustering algorithm from scratch. A GMM distribution is composed of `k` components, each characterized by:\n",
        "\n",
        "1. A mixture proportion\n",
        "2. A mean for its Normal Distribution\n",
        "3. A variance for its Normal Distribution\n",
        "\n",
        "So, to generate a dataset that follows a GMM distrbution we need a list of those parameters. In this exercise we will use a class called `Component` to capture the parameters for a given component. And a GMM will be a list of `Component`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Component:\n",
        "    def __init__(self, mixture_prop, mean, variance):\n",
        "        self.mixture_prop = mixture_prop\n",
        "        self.mean = mean\n",
        "        self.variance = variance\n",
        "\n",
        "example_gmm = [Component(.5, 5, 1), Component(.5, 8, 1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a) Complete the function below to validate and generate a dataset following a GMM distribution, given a specified set of GMM parameters as above and a size. You may only use the methods already imported in the cell. (10pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.random import normal, uniform\n",
        "\n",
        "def generate_gmm_dataset(gmm_params, size):\n",
        "    if not is_valid_gmm(gmm_params):\n",
        "        raise ValueError(\"GMM parameters are invalid\")\n",
        "    \n",
        "    dataset = []\n",
        "    for _ in range(size):\n",
        "        comp = get_random_component(gmm_params)\n",
        "        dataset += ...\n",
        "    return dataset\n",
        "\n",
        "def is_valid_gmm(gmm_params):\n",
        "    '''\n",
        "        Checks that the sum of the mixture\n",
        "        proportions is 1\n",
        "    '''\n",
        "    return True\n",
        "\n",
        "def get_random_component(gmm_params):\n",
        "    '''\n",
        "        returns component with prob\n",
        "        proportional to mixture_prop\n",
        "    '''\n",
        "    ...\n",
        "    return\n",
        "\n",
        "# test your code: this should return a list of 10 numbers similar to worksheet 8\n",
        "data = generate_gmm_dataset(example_gmm, 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b) Finish the implementation below of the Expectation-Maximization Algorithm. Only use methods that have been imported in the cell. Visualize the output of your code by plotting the original mixture distribution curves and the ones learned by the EM algorithm. (15pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "from numpy import array, argmax\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def gmm_init(k, dataset):\n",
        "    kmeans = KMeans(k, init='k-means++').fit(X=array(dataset).reshape(-1, 1))\n",
        "    gmm_params = []\n",
        "    ...\n",
        "    return gmm_params\n",
        "\n",
        "\n",
        "def compute_gmm(k, dataset, probs):\n",
        "    '''\n",
        "        Compute P(C_j), mean_j, var_j\n",
        "    '''\n",
        "    gmm_params = []\n",
        "    ...\n",
        "    return gmm_params\n",
        "\n",
        "\n",
        "def compute_probs(k, dataset, gmm_params):\n",
        "    '''\n",
        "        For all x_i in dataset, compute P(C_j | X_i)\n",
        "        = P(X_i | C_j)P(C_j) / P(X_i) for all C_j\n",
        "        return the list of lists of all P(C_j | X_i)\n",
        "        for all x_i in dataset.\n",
        "    '''\n",
        "    probs = []\n",
        "    ...\n",
        "    return probs\n",
        "\n",
        "\n",
        "def expectation_maximization(k, dataset, iterations):\n",
        "    '''\n",
        "        Repeat for a set number of iterations.\n",
        "    '''\n",
        "    gmm_params = gmm_init(k, dataset)\n",
        "    for _ in range(iterations):\n",
        "        # expectation step\n",
        "        probs = compute_probs(k, dataset, gmm_params)\n",
        "\n",
        "        # maximization step\n",
        "        gmm_params = compute_gmm(k, dataset, probs)\n",
        "\n",
        "    return probs, gmm_params"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes:\n",
        "\n",
        "1. your code should work with any number of components, each with reasonable parameters.\n",
        "2. your code should work for 1 to about 5 iterations of the EM algorithm. It may not work for iterations over 10 because the math we are doing may overflow and create `nans` - that's ok / don't worry about it."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
